import os
import tempfile
from pathlib import Path
import streamlit as st

from resume_processor import (
    load_resume,
    analyze_resume,
    store_to_vectorstore,
    run_self_query,
    compute_fit_scores,
    store_job_description,
)

st.set_page_config(page_title="AI Resume Screener", page_icon="üß†", layout="wide")
st.title("AI Resume Screener")
st.caption("Upload a resume, analyze it against a job description, filter relevant chunks, and search across stored resumes.")

# --- Key check (works with Streamlit Secrets or local .env) ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")
if not OPENAI_API_KEY:
    st.warning("OpenAI API key not found. Add it in Streamlit Secrets or .env for full functionality.")

# --- Inputs ---
job_title = st.text_input("üìù Job Title / Label", value="Software Engineer")
job_desc = st.text_area("üìã Paste Job Description", height=200, placeholder="Paste the job description here...")

uploaded_file = st.file_uploader("üìé Upload Resume (PDF, DOCX, or TXT)", type=["pdf", "docx", "txt"])

st.markdown("### Storage Options")
col_filter1, col_filter2, col_filter3 = st.columns([1,1,1], vertical_alignment="center")

with col_filter1:
    only_best = st.checkbox("Store only best chunks (by similarity to JD)", value=True)
with col_filter2:
    top_k = st.slider("Top-K chunks to keep", min_value=1, max_value=40, value=8, step=1, disabled=not only_best)
with col_filter3:
    min_sim = st.slider("Min similarity (0‚Äì1)", min_value=0.00, max_value=1.00, value=0.00, step=0.01, disabled=not only_best)

if not only_best:
    top_k = 0
    min_sim = None

colA, colB = st.columns([1, 1])

with colA:
    analyze_clicked = st.button("Analyze & Store", use_container_width=True)

if analyze_clicked and uploaded_file and job_desc:
    # Save uploaded file to a temp path
    with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{uploaded_file.name}") as tmp:
        tmp.write(uploaded_file.getbuffer())
        tmp_path = tmp.name

    with st.spinner("Analyzing resume..."):
        docs = load_resume(tmp_path)
        chunk_analyses, final_report = analyze_resume(docs, job_desc)

    # Optional: store job description (for organization/traceability)
    store_job_description(job_desc, label=job_title, persist_directory="chroma_jd")

    # Compute vector-based fit score vs JD
    fit = compute_fit_scores(job_desc, docs, top_k=5)

    # Store resume chunks with optional filtering
    with st.spinner("Storing resume chunks..."):
        stats = store_to_vectorstore(
            docs,
            persist_directory="chroma_store",
            filter_job_description=job_desc if only_best else None,
            top_k=top_k if only_best else 0,
            min_sim=min_sim if only_best else None,
        )

    st.success("‚úÖ Analysis complete and stored.")
    st.subheader("üìä Vector Fit Score")
    col1, col2, col3 = st.columns(3)
    col1.metric("Suitability (vector, 0‚Äì100)", fit["vector_suitability_0_100"])
    col2.metric("Max similarity", f"{fit['max_sim']:.3f}")
    col3.metric("Avg top-K similarity", f"{fit['avg_top_k_sim']:.3f}")

    with st.expander("Top matches (vector search)", expanded=False):
        for i, (snippet, sim) in enumerate(fit["top_matches"], 1):
            st.markdown(f"**Match {i} ‚Äî sim {sim:.3f}**")
            st.write(snippet.strip())
            st.divider()

    st.subheader("üìÑ Final Consolidated Report")
    st.write(final_report)
    st.download_button("üì• Download Report", final_report, file_name=f"{job_title.replace(' ','_')}_resume_analysis.txt")

    with st.expander("Per-chunk analyses (raw)", expanded=False):
        for i, txt in enumerate(chunk_analyses, 1):
            st.markdown(f"**Chunk {i}**")
            st.write(txt)
            st.divider()

st.divider()
st.subheader("üîé Ask Anything About Stored Resumes")
query = st.text_input("Enter a smart query (e.g., 'Python developer with AWS and Kubernetes')")
if st.button("Search Resumes", use_container_width=True) and query:
    with st.spinner("Searching..."):
        results = run_self_query(query, persist_directory="chroma_store", k=5)
    if results:
        for i, res in enumerate(results, 1):
            st.markdown(f"**Result {i}**")
            st.write(res.page_content.strip())
            meta = res.metadata or {}
            if meta:
                sim_txt = f" | sim‚âà{meta.get('jd_similarity'):.3f}" if "jd_similarity" in meta else ""
                st.caption(f"Source: {meta.get('source','unknown')}{sim_txt}")
            st.divider()
    else:
        st.warning("No matches found. Try a broader query or upload more resumes.")
